{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = pd.read_csv('./data/judge-1377884607_tweet_product_company.csv',encoding='ISO-8859-1')\n",
    "sentiments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments.dropna(subset=['tweet_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    sw = stopwords.words('english')\n",
    "    text = [str(phrase).lower() for phrase in text]\n",
    "    # tokenizer_ = TweetTokenizer(preserve_case=False)\n",
    "    tokenizer_ = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    tokens = [' '.join(tokenizer_.tokenize(str(doc))) for doc in text]\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msmooth_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Transform a count matrix to a normalized tf or tf-idf representation\n",
      "\n",
      "Tf means term-frequency while tf-idf means term-frequency times inverse\n",
      "document-frequency. This is a common term weighting scheme in information\n",
      "retrieval, that has also found good use in document classification.\n",
      "\n",
      "The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
      "token in a given document is to scale down the impact of tokens that occur\n",
      "very frequently in a given corpus and that are hence empirically less\n",
      "informative than features that occur in a small fraction of the training\n",
      "corpus.\n",
      "\n",
      "The formula that is used to compute the tf-idf for a term t of a document d\n",
      "in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
      "computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
      "n is the total number of documents in the document set and df(t) is the\n",
      "document frequency of t; the document frequency is the number of documents\n",
      "in the document set that contain the term t. The effect of adding \"1\" to\n",
      "the idf in the equation above is that terms with zero idf, i.e., terms\n",
      "that occur in all documents in a training set, will not be entirely\n",
      "ignored.\n",
      "(Note that the idf formula above differs from the standard textbook\n",
      "notation that defines the idf as\n",
      "idf(t) = log [ n / (df(t) + 1) ]).\n",
      "\n",
      "If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
      "numerator and denominator of the idf as if an extra document was seen\n",
      "containing every term in the collection exactly once, which prevents\n",
      "zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n",
      "\n",
      "Furthermore, the formulas used to compute tf and idf depend\n",
      "on parameter settings that correspond to the SMART notation used in IR\n",
      "as follows:\n",
      "\n",
      "Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
      "``sublinear_tf=True``.\n",
      "Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
      "Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
      "when ``norm=None``.\n",
      "\n",
      "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "norm : {'l1', 'l2'}, default='l2'\n",
      "    Each output row will have unit norm, either:\n",
      "    * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "    similarity between two vectors is their dot product when l2 norm has\n",
      "    been applied.\n",
      "    * 'l1': Sum of absolute values of vector elements is 1.\n",
      "    See :func:`preprocessing.normalize`\n",
      "\n",
      "use_idf : bool, default=True\n",
      "    Enable inverse-document-frequency reweighting.\n",
      "\n",
      "smooth_idf : bool, default=True\n",
      "    Smooth idf weights by adding one to document frequencies, as if an\n",
      "    extra document was seen containing every term in the collection\n",
      "    exactly once. Prevents zero divisions.\n",
      "\n",
      "sublinear_tf : bool, default=False\n",
      "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "idf_ : array of shape (n_features)\n",
      "    The inverse document frequency (IDF) vector; only defined\n",
      "    if  ``use_idf`` is True.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.feature_extraction.text import TfidfTransformer\n",
      ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
      ">>> from sklearn.pipeline import Pipeline\n",
      ">>> import numpy as np\n",
      ">>> corpus = ['this is the first document',\n",
      "...           'this document is the second document',\n",
      "...           'and this is the third one',\n",
      "...           'is this the first document']\n",
      ">>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
      "...               'and', 'one']\n",
      ">>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
      "...                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
      ">>> pipe['count'].transform(corpus).toarray()\n",
      "array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
      "       [1, 2, 0, 1, 1, 1, 0, 0],\n",
      "       [1, 0, 0, 1, 0, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 0, 1, 0, 0]])\n",
      ">>> pipe['tfid'].idf_\n",
      "array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
      "       1.        , 1.91629073, 1.91629073])\n",
      ">>> pipe.transform(corpus).shape\n",
      "(4, 8)\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      ".. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n",
      "               Information Retrieval. Addison Wesley, pp. 68-74.\n",
      "\n",
      ".. [MRS2008] C.D. Manning, P. Raghavan and H. Schütze  (2008).\n",
      "               Introduction to Information Retrieval. Cambridge University\n",
      "               Press, pp. 118-120.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "TfidfTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "# cvr = CountVectorizer(\n",
    "#     encoding='ISO-8859-1',\n",
    "#     stop_words=sw,\n",
    "#     ngram_range=[2,4],\n",
    "#     min_df=4,max_df=50\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = sentiments[\n",
    "                (sentiments[emotion] == 'Positive emotion') |\n",
    "                (sentiments[emotion] == 'Negative emotion')\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2978\n",
       "Negative emotion     570\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n[emotion].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = cvr.fit_transform(sentiments['tweet_text'])\n",
    "# tokens = tokenize(sentiments['tweet_text'])\n",
    "tokens = tokenize(y_n['tweet_text'])\n",
    "# X = cvr.fit_transform(tokens)\n",
    "# X\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem = []\n",
    "for doc in tokens:\n",
    "    lem.append(' '.join([lemmatizer.lemmatize(token) for token in doc.split(' ')]))\n",
    "tfidf = TfidfVectorizer(ngram_range=[1,4],max_df=0.5,min_df=10)\n",
    "# print(tokens)\n",
    "X = tfidf.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>about to</th>\n",
       "      <th>aclu</th>\n",
       "      <th>action</th>\n",
       "      <th>action link</th>\n",
       "      <th>actually</th>\n",
       "      <th>ad</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>...</th>\n",
       "      <th>you speak quot</th>\n",
       "      <th>you speak quot mark</th>\n",
       "      <th>you tweet</th>\n",
       "      <th>you tweet quot</th>\n",
       "      <th>you tweet quot is</th>\n",
       "      <th>your</th>\n",
       "      <th>your ipad</th>\n",
       "      <th>your iphone</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zazzlesxsw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  about the  about to  aclu  action  action link  actually   ad  \\\n",
       "0  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "1  0.196329        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "2  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "3  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "4  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "\n",
       "      after  again  ...  you speak quot  you speak quot mark  you tweet  \\\n",
       "0  0.348367    0.0  ...             0.0                  0.0        0.0   \n",
       "1  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "2  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "3  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "4  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "\n",
       "   you tweet quot  you tweet quot is  your  your ipad  your iphone  yrs  \\\n",
       "0             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "1             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "2             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "3             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "4             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "\n",
       "   zazzlesxsw  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "\n",
       "[5 rows x 1787 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = pd.DataFrame(X.toarray(),columns=tfidf.get_feature_names())\n",
    "vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwhiten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msvd_solver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0miterated_power\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Principal component analysis (PCA).\n",
      "\n",
      "Linear dimensionality reduction using Singular Value Decomposition of the\n",
      "data to project it to a lower dimensional space. The input data is centered\n",
      "but not scaled for each feature before applying the SVD.\n",
      "\n",
      "It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
      "SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
      "data and the number of components to extract.\n",
      "\n",
      "It can also use the scipy.sparse.linalg ARPACK implementation of the\n",
      "truncated SVD.\n",
      "\n",
      "Notice that this class does not support sparse input. See\n",
      ":class:`TruncatedSVD` for an alternative with sparse data.\n",
      "\n",
      "Read more in the :ref:`User Guide <PCA>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_components : int, float, None or str\n",
      "    Number of components to keep.\n",
      "    if n_components is not set all components are kept::\n",
      "\n",
      "        n_components == min(n_samples, n_features)\n",
      "\n",
      "    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n",
      "    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n",
      "    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n",
      "\n",
      "    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n",
      "    number of components such that the amount of variance that needs to be\n",
      "    explained is greater than the percentage specified by n_components.\n",
      "\n",
      "    If ``svd_solver == 'arpack'``, the number of components must be\n",
      "    strictly less than the minimum of n_features and n_samples.\n",
      "\n",
      "    Hence, the None case results in::\n",
      "\n",
      "        n_components == min(n_samples, n_features) - 1\n",
      "\n",
      "copy : bool, default=True\n",
      "    If False, data passed to fit are overwritten and running\n",
      "    fit(X).transform(X) will not yield the expected results,\n",
      "    use fit_transform(X) instead.\n",
      "\n",
      "whiten : bool, optional (default False)\n",
      "    When True (False by default) the `components_` vectors are multiplied\n",
      "    by the square root of n_samples and then divided by the singular values\n",
      "    to ensure uncorrelated outputs with unit component-wise variances.\n",
      "\n",
      "    Whitening will remove some information from the transformed signal\n",
      "    (the relative variance scales of the components) but can sometime\n",
      "    improve the predictive accuracy of the downstream estimators by\n",
      "    making their data respect some hard-wired assumptions.\n",
      "\n",
      "svd_solver : str {'auto', 'full', 'arpack', 'randomized'}\n",
      "    If auto :\n",
      "        The solver is selected by a default policy based on `X.shape` and\n",
      "        `n_components`: if the input data is larger than 500x500 and the\n",
      "        number of components to extract is lower than 80% of the smallest\n",
      "        dimension of the data, then the more efficient 'randomized'\n",
      "        method is enabled. Otherwise the exact full SVD is computed and\n",
      "        optionally truncated afterwards.\n",
      "    If full :\n",
      "        run exact full SVD calling the standard LAPACK solver via\n",
      "        `scipy.linalg.svd` and select the components by postprocessing\n",
      "    If arpack :\n",
      "        run SVD truncated to n_components calling ARPACK solver via\n",
      "        `scipy.sparse.linalg.svds`. It requires strictly\n",
      "        0 < n_components < min(X.shape)\n",
      "    If randomized :\n",
      "        run randomized SVD by the method of Halko et al.\n",
      "\n",
      "    .. versionadded:: 0.18.0\n",
      "\n",
      "tol : float >= 0, optional (default .0)\n",
      "    Tolerance for singular values computed by svd_solver == 'arpack'.\n",
      "\n",
      "    .. versionadded:: 0.18.0\n",
      "\n",
      "iterated_power : int >= 0, or 'auto', (default 'auto')\n",
      "    Number of iterations for the power method computed by\n",
      "    svd_solver == 'randomized'.\n",
      "\n",
      "    .. versionadded:: 0.18.0\n",
      "\n",
      "random_state : int, RandomState instance, default=None\n",
      "    Used when ``svd_solver`` == 'arpack' or 'randomized'. Pass an int\n",
      "    for reproducible results across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "    .. versionadded:: 0.18.0\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "components_ : array, shape (n_components, n_features)\n",
      "    Principal axes in feature space, representing the directions of\n",
      "    maximum variance in the data. The components are sorted by\n",
      "    ``explained_variance_``.\n",
      "\n",
      "explained_variance_ : array, shape (n_components,)\n",
      "    The amount of variance explained by each of the selected components.\n",
      "\n",
      "    Equal to n_components largest eigenvalues\n",
      "    of the covariance matrix of X.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "\n",
      "explained_variance_ratio_ : array, shape (n_components,)\n",
      "    Percentage of variance explained by each of the selected components.\n",
      "\n",
      "    If ``n_components`` is not set then all components are stored and the\n",
      "    sum of the ratios is equal to 1.0.\n",
      "\n",
      "singular_values_ : array, shape (n_components,)\n",
      "    The singular values corresponding to each of the selected components.\n",
      "    The singular values are equal to the 2-norms of the ``n_components``\n",
      "    variables in the lower-dimensional space.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "\n",
      "mean_ : array, shape (n_features,)\n",
      "    Per-feature empirical mean, estimated from the training set.\n",
      "\n",
      "    Equal to `X.mean(axis=0)`.\n",
      "\n",
      "n_components_ : int\n",
      "    The estimated number of components. When n_components is set\n",
      "    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n",
      "    number is estimated from input data. Otherwise it equals the parameter\n",
      "    n_components, or the lesser value of n_features and n_samples\n",
      "    if n_components is None.\n",
      "\n",
      "n_features_ : int\n",
      "    Number of features in the training data.\n",
      "\n",
      "n_samples_ : int\n",
      "    Number of samples in the training data.\n",
      "\n",
      "noise_variance_ : float\n",
      "    The estimated noise covariance following the Probabilistic PCA model\n",
      "    from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
      "    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
      "    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
      "    compute the estimated data covariance and score samples.\n",
      "\n",
      "    Equal to the average of (min(n_features, n_samples) - n_components)\n",
      "    smallest eigenvalues of the covariance matrix of X.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "KernelPCA : Kernel Principal Component Analysis.\n",
      "SparsePCA : Sparse Principal Component Analysis.\n",
      "TruncatedSVD : Dimensionality reduction using truncated SVD.\n",
      "IncrementalPCA : Incremental Principal Component Analysis.\n",
      "\n",
      "References\n",
      "----------\n",
      "For n_components == 'mle', this class uses the method of *Minka, T. P.\n",
      "\"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n",
      "\n",
      "Implements the probabilistic PCA model from:\n",
      "Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n",
      "component analysis\". Journal of the Royal Statistical Society:\n",
      "Series B (Statistical Methodology), 61(3), 611-622.\n",
      "via the score and score_samples methods.\n",
      "See http://www.miketipping.com/papers/met-mppca.pdf\n",
      "\n",
      "For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n",
      "\n",
      "For svd_solver == 'randomized', see:\n",
      "*Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n",
      "\"Finding structure with randomness: Probabilistic algorithms for\n",
      "constructing approximate matrix decompositions\".\n",
      "SIAM review, 53(2), 217-288.* and also\n",
      "*Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n",
      "\"A randomized algorithm for the decomposition of matrices\".\n",
      "Applied and Computational Harmonic Analysis, 30(1), 47-68.*\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.decomposition import PCA\n",
      ">>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      ">>> pca = PCA(n_components=2)\n",
      ">>> pca.fit(X)\n",
      "PCA(n_components=2)\n",
      ">>> print(pca.explained_variance_ratio_)\n",
      "[0.9924... 0.0075...]\n",
      ">>> print(pca.singular_values_)\n",
      "[6.30061... 0.54980...]\n",
      "\n",
      ">>> pca = PCA(n_components=2, svd_solver='full')\n",
      ">>> pca.fit(X)\n",
      "PCA(n_components=2, svd_solver='full')\n",
      ">>> print(pca.explained_variance_ratio_)\n",
      "[0.9924... 0.00755...]\n",
      ">>> print(pca.singular_values_)\n",
      "[6.30061... 0.54980...]\n",
      "\n",
      ">>> pca = PCA(n_components=1, svd_solver='arpack')\n",
      ">>> pca.fit(X)\n",
      "PCA(n_components=1, svd_solver='arpack')\n",
      ">>> print(pca.explained_variance_ratio_)\n",
      "[0.99244...]\n",
      ">>> print(pca.singular_values_)\n",
      "[6.30061...]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion  = 'is_there_an_emotion_directed_at_a_brand_or_product'\n",
    "directed = 'emotion_in_tweet_is_directed_at'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02828757, -0.03904504, -0.05439692, ...,  0.04031826,\n",
       "         0.04961264,  0.00352633],\n",
       "       [-0.03091112, -0.11160592, -0.09518222, ..., -0.02816939,\n",
       "        -0.02183693, -0.02596585],\n",
       "       [-0.03269821, -0.01290945, -0.01433204, ...,  0.00458921,\n",
       "         0.03887284,  0.02908574],\n",
       "       ...,\n",
       "       [ 0.04327187, -0.11066595,  0.20276061, ...,  0.06856005,\n",
       "        -0.04172619,  0.00923729],\n",
       "       [-0.04099579, -0.09701641, -0.10548752, ...,  0.0186417 ,\n",
       "         0.01613704,  0.02167043],\n",
       "       [-0.018109  ,  0.06210292, -0.06842777, ...,  0.01494416,\n",
       "        -0.03127936,  0.042141  ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit_transform(vect,y_n[emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01578003, 0.02927832, 0.04040168, 0.05044328, 0.059613  ,\n",
       "       0.06852648, 0.07684327, 0.08469489, 0.09214256, 0.09922066,\n",
       "       0.10595196, 0.11239323, 0.11857932, 0.12449319, 0.13020882,\n",
       "       0.13586541, 0.14130069, 0.14656404, 0.15173897, 0.15685874,\n",
       "       0.16180026, 0.16669367, 0.17144812, 0.17607212, 0.1806048 ,\n",
       "       0.18505464, 0.18949031, 0.19378023, 0.19799648, 0.20214138,\n",
       "       0.20623384, 0.2102634 , 0.21426768, 0.21817088, 0.22201055,\n",
       "       0.22584845, 0.22963628, 0.23333933, 0.2369358 , 0.24051295,\n",
       "       0.24404324, 0.24753798, 0.25093855, 0.25429719, 0.25762989,\n",
       "       0.26092445, 0.26417073, 0.26740517, 0.27060985, 0.27381136,\n",
       "       0.2769089 , 0.27998402, 0.28303414, 0.28607232, 0.28908969,\n",
       "       0.29208116, 0.2950002 , 0.29790107, 0.30078542, 0.30364204,\n",
       "       0.30648674, 0.30930475, 0.31209992, 0.31488185, 0.31764932,\n",
       "       0.32040468, 0.32310006, 0.32577996, 0.32843086, 0.33107027,\n",
       "       0.33368625, 0.33629375, 0.33889492, 0.34146573, 0.34402409,\n",
       "       0.34654836, 0.34904861, 0.3515079 , 0.35395348, 0.356391  ,\n",
       "       0.3588029 , 0.36120355, 0.36359244, 0.36595406, 0.36830304,\n",
       "       0.37063515, 0.37294484, 0.3752418 , 0.37753474, 0.37981257,\n",
       "       0.38205835, 0.38428369, 0.38648891, 0.38868633, 0.39086851,\n",
       "       0.39304736, 0.39519694, 0.39733729, 0.39946431, 0.40156049,\n",
       "       0.40363528, 0.40570502, 0.40775622, 0.40980381, 0.4118285 ,\n",
       "       0.41384222, 0.41585112, 0.41785206, 0.41984706, 0.42182529,\n",
       "       0.42379431, 0.42575636, 0.4277083 , 0.42964562, 0.43157718,\n",
       "       0.43348574, 0.43538738, 0.43727608, 0.4391601 , 0.44103283,\n",
       "       0.44289145, 0.44473148, 0.44656448, 0.44839038, 0.45021378,\n",
       "       0.45202631, 0.45383757, 0.45563553, 0.45742313, 0.45921002,\n",
       "       0.46098404, 0.46274367, 0.46449195, 0.46622747, 0.46795824,\n",
       "       0.46968384, 0.47139627, 0.47310641, 0.47481267, 0.47650813,\n",
       "       0.47819966, 0.47988034, 0.48155828, 0.48321955, 0.48487284,\n",
       "       0.48651622, 0.48814484, 0.48977073, 0.49139385, 0.49300047,\n",
       "       0.49460608, 0.49620291, 0.49778815, 0.49937114, 0.50094183,\n",
       "       0.50250563, 0.50406761, 0.50562222, 0.50716637, 0.50870725,\n",
       "       0.51023986, 0.51175806, 0.51327512, 0.51478947, 0.51629332,\n",
       "       0.51778734, 0.51927727, 0.52076209, 0.52224601, 0.52372771,\n",
       "       0.52520096, 0.52666441, 0.52811408, 0.52955821, 0.53099501,\n",
       "       0.53242845, 0.53385623, 0.53527907, 0.53669661, 0.53810534,\n",
       "       0.53950915, 0.54091011, 0.5423038 , 0.54369133, 0.54507752,\n",
       "       0.54645103, 0.54782075, 0.54918724, 0.5505497 , 0.55190846,\n",
       "       0.55326208, 0.5546105 , 0.55595162, 0.55728612, 0.55861688,\n",
       "       0.55994567, 0.5612669 , 0.56258539, 0.56389782, 0.565207  ,\n",
       "       0.56650918, 0.56780887, 0.56910351, 0.57039721, 0.57168294,\n",
       "       0.57296135, 0.57423438, 0.57550384, 0.57676979, 0.57803208,\n",
       "       0.57928999, 0.58054146, 0.58179108, 0.58303805, 0.58427914,\n",
       "       0.58551455, 0.58674819, 0.58797108, 0.58918598, 0.59039826,\n",
       "       0.5916086 , 0.59281767, 0.59402428, 0.59522219, 0.59641707,\n",
       "       0.59760979, 0.59879792, 0.59998407, 0.60116504, 0.60234414,\n",
       "       0.60351472, 0.6046842 , 0.60585009, 0.60701379, 0.60817138,\n",
       "       0.60932491, 0.61047577, 0.6116239 , 0.61277076, 0.6139076 ,\n",
       "       0.6150433 , 0.61617725, 0.61730586, 0.61843011, 0.61954974,\n",
       "       0.62066466, 0.62177683, 0.62288773, 0.62399376, 0.62509607,\n",
       "       0.62619381, 0.62728867, 0.62838251, 0.62947145, 0.63055446,\n",
       "       0.63163448, 0.63271297, 0.63378846, 0.63486247, 0.63593362,\n",
       "       0.63699869, 0.6380605 , 0.63911896, 0.64017409, 0.6412275 ,\n",
       "       0.64227602, 0.64332243, 0.64436744, 0.64540805, 0.64644401,\n",
       "       0.6474766 , 0.64850767, 0.649537  , 0.65056208, 0.65158384,\n",
       "       0.65259918, 0.65361295, 0.65462445, 0.65563205, 0.65663782,\n",
       "       0.65763754, 0.65863526, 0.65963021, 0.66062151, 0.6616086 ,\n",
       "       0.66259553, 0.66357557, 0.66455497, 0.66553262, 0.66650907,\n",
       "       0.66748225, 0.66845324, 0.66942127, 0.67038364, 0.67134139,\n",
       "       0.67229865, 0.67325159, 0.67420122, 0.67514526, 0.67608774,\n",
       "       0.67702896, 0.67796728, 0.67890226, 0.67983585, 0.68076334,\n",
       "       0.68169   , 0.68261045, 0.68352813, 0.68444362, 0.68535816,\n",
       "       0.68627161, 0.6871814 , 0.68808647, 0.68899115, 0.68989367,\n",
       "       0.69079096, 0.69168658, 0.69257973, 0.69347096, 0.69436129,\n",
       "       0.69524639, 0.6961296 , 0.69701093, 0.69789004, 0.69876841,\n",
       "       0.69964232, 0.7005154 , 0.7013832 , 0.70225   , 0.70311479,\n",
       "       0.70397853, 0.70483848, 0.70569693, 0.70655127, 0.7074021 ,\n",
       "       0.7082507 , 0.70909713, 0.70993972, 0.71078038, 0.71162012,\n",
       "       0.71245797, 0.71329256, 0.71412526, 0.71495664, 0.71578747,\n",
       "       0.71661367, 0.71743758, 0.71826128, 0.71908012, 0.71989602,\n",
       "       0.72070964, 0.72151942, 0.72232717, 0.72313247, 0.72393544,\n",
       "       0.72473801, 0.72553866, 0.7263381 , 0.7271338 , 0.72792716,\n",
       "       0.72871862, 0.72950826, 0.73029563, 0.73108094, 0.73186171,\n",
       "       0.73264225, 0.73342029, 0.73419548, 0.73496918, 0.73574129,\n",
       "       0.73651095, 0.73727885, 0.73804421, 0.73880661, 0.73956873,\n",
       "       0.74032879, 0.74108646, 0.74184193, 0.74259646, 0.74334925,\n",
       "       0.74409848, 0.74484691, 0.74559371, 0.74633703, 0.7470776 ,\n",
       "       0.74781723, 0.74855221, 0.74928577, 0.75001735, 0.75074711,\n",
       "       0.75147499, 0.75220184, 0.75292494, 0.75364655, 0.75436452,\n",
       "       0.75508023, 0.75579369, 0.75650692, 0.75721897, 0.75792793,\n",
       "       0.75863545, 0.75934179, 0.7600458 , 0.76074895, 0.76144887,\n",
       "       0.76214635, 0.7628415 , 0.76353598, 0.76422987, 0.76492207,\n",
       "       0.76561206, 0.76629939, 0.7669845 , 0.76766845, 0.76835065,\n",
       "       0.76903193, 0.76971205, 0.77039087, 0.77106601, 0.77173985,\n",
       "       0.77241275, 0.77308351, 0.77375216, 0.7744197 , 0.77508448,\n",
       "       0.77574864, 0.77641111, 0.77707189, 0.77773044, 0.77838636,\n",
       "       0.77904113, 0.77969354, 0.78034514, 0.78099371, 0.78164163,\n",
       "       0.78228574, 0.78292809, 0.78356939, 0.78420981, 0.78484874,\n",
       "       0.78548542, 0.78612009, 0.78675352, 0.78738613, 0.78801657,\n",
       "       0.7886461 , 0.78927384, 0.78989977, 0.79052352, 0.7911462 ,\n",
       "       0.79176763, 0.7923878 , 0.79300612, 0.79362249, 0.79423557,\n",
       "       0.79484755, 0.79545674, 0.79606499, 0.7966715 , 0.79727752,\n",
       "       0.79788214, 0.79848455, 0.79908587, 0.79968648, 0.80028464,\n",
       "       0.80088087, 0.801476  , 0.80207038, 0.80266335, 0.80325336,\n",
       "       0.80384103, 0.80442701, 0.80501144, 0.80559407, 0.80617639,\n",
       "       0.8067576 , 0.80733715, 0.80791521, 0.80849185, 0.80906586,\n",
       "       0.80963962, 0.81021119, 0.81078119, 0.81134832, 0.81191491,\n",
       "       0.81248027, 0.81304427, 0.81360748, 0.8141699 , 0.81473057,\n",
       "       0.81528972, 0.8158464 , 0.81640175, 0.8169552 , 0.81750771,\n",
       "       0.81805952, 0.818609  , 0.81915828, 0.81970625, 0.82025379,\n",
       "       0.82079847, 0.82134105, 0.82188347, 0.82242376, 0.82296377,\n",
       "       0.82350241, 0.82403943, 0.82457513, 0.82511029, 0.82564336,\n",
       "       0.82617548, 0.82670552, 0.82723518, 0.82776307, 0.8282898 ,\n",
       "       0.82881514, 0.82933978, 0.829863  , 0.83038551, 0.83090593,\n",
       "       0.8314246 , 0.83194143, 0.83245692, 0.83297162, 0.83348485,\n",
       "       0.83399737, 0.83450801, 0.83501857, 0.83552783, 0.83603476,\n",
       "       0.83654015, 0.83704486, 0.83754837, 0.83805141, 0.83855249,\n",
       "       0.83905274, 0.83955266, 0.8400516 , 0.84054923, 0.84104514,\n",
       "       0.84153894, 0.8420321 , 0.84252301, 0.84301274, 0.8435015 ,\n",
       "       0.84398875, 0.8444746 , 0.84495936, 0.84544367, 0.845926  ,\n",
       "       0.84640615, 0.84688604, 0.84736397, 0.84784045, 0.84831576,\n",
       "       0.84879017, 0.84926335, 0.84973518, 0.85020636])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1   @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3   @sxsw I hope this year's festival isn't as cra...   \n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5   @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "7   #SXSW is just starting, #CTIA is around the co...   \n",
       "8   Beautifully smart and simple idea RT @madebyma...   \n",
       "9   Counting down the days to #sxsw plus strong Ca...   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...   \n",
       "\n",
       "   emotion_in_tweet_is_directed_at  \\\n",
       "0                           iPhone   \n",
       "1               iPad or iPhone App   \n",
       "2                             iPad   \n",
       "3               iPad or iPhone App   \n",
       "4                           Google   \n",
       "5                              NaN   \n",
       "7                          Android   \n",
       "8               iPad or iPhone App   \n",
       "9                            Apple   \n",
       "10                         Android   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                    Negative emotion  \n",
       "1                                    Positive emotion  \n",
       "2                                    Positive emotion  \n",
       "3                                    Negative emotion  \n",
       "4                                    Positive emotion  \n",
       "5                  No emotion toward brand or product  \n",
       "7                                    Positive emotion  \n",
       "8                                    Positive emotion  \n",
       "9                                    Positive emotion  \n",
       "10                                   Positive emotion  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694    @mention #qagb with @mention Listening to @men...\n",
       "1376    Apple pop-up shop for #SXSW. How cool is that?...\n",
       "518     Thank you for these! RT @mention Public notes ...\n",
       "6242    RT @mention Leaving Austin and I sort of want ...\n",
       "8881    I'll be tweeting out #SXSW sessions I'm going ...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments.sample(5)['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save me some cash! TechCrunch Giveaway: An iPad 2åÊ#TechCrunch {link} via @mention #winning! #ipad2 #sxsw #apple'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments.iloc[5014]['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5388\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9092 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 284.1+ KB\n"
     ]
    }
   ],
   "source": [
    "for col in ['emotion_in_tweet_is_directed_at','is_there_an_emotion_directed_at_a_brand_or_product']:\n",
    "    display(sentiments[col].value_counts())\n",
    "sentiments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_there_an_emotion_directed_at_a_brand_or_product\n",
       "No emotion toward brand or product    5297\n",
       "Positive emotion                       306\n",
       "I can't tell                           147\n",
       "Negative emotion                        51\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiments.isna()\n",
    "vals_ = sentiments[sentiments['emotion_in_tweet_is_directed_at'].isna()].value_counts('is_there_an_emotion_directed_at_a_brand_or_product')\n",
    "vals_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweet_text, emotion_in_tweet_is_directed_at, is_there_an_emotion_directed_at_a_brand_or_product]\n",
       "Index: []"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[sentiments['tweet_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals_.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>about to</th>\n",
       "      <th>aclu</th>\n",
       "      <th>action</th>\n",
       "      <th>action link</th>\n",
       "      <th>actually</th>\n",
       "      <th>ad</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>...</th>\n",
       "      <th>you speak quot</th>\n",
       "      <th>you speak quot mark</th>\n",
       "      <th>you tweet</th>\n",
       "      <th>you tweet quot</th>\n",
       "      <th>you tweet quot is</th>\n",
       "      <th>your</th>\n",
       "      <th>your ipad</th>\n",
       "      <th>your iphone</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zazzlesxsw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  about the  about to  aclu  action  action link  actually   ad  \\\n",
       "0  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "1  0.196329        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "2  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "3  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "4  0.000000        0.0       0.0   0.0     0.0          0.0       0.0  0.0   \n",
       "\n",
       "      after  again  ...  you speak quot  you speak quot mark  you tweet  \\\n",
       "0  0.348367    0.0  ...             0.0                  0.0        0.0   \n",
       "1  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "2  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "3  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "4  0.000000    0.0  ...             0.0                  0.0        0.0   \n",
       "\n",
       "   you tweet quot  you tweet quot is  your  your ipad  your iphone  yrs  \\\n",
       "0             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "1             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "2             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "3             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "4             0.0                0.0   0.0        0.0          0.0  0.0   \n",
       "\n",
       "   zazzlesxsw  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "\n",
       "[5 rows x 1787 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
